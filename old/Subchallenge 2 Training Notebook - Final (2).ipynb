{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f863512a",
   "metadata": {},
   "source": [
    "# Subchallenge Challenge 2 Training notebook\n",
    "\n",
    "* Use only solicited for training and validation\n",
    "* Use BCE for loss function\n",
    "* Scaled images to 0-1\n",
    "* Used frequencies from approx 50 Hz to 15000Hz\n",
    "* inference on test set is per patient, not per cough.  provide confidence score\n",
    "\n",
    "* here we use standard spectrum bands similar to paper below.\n",
    "\n",
    "https://www.researchgate.net/publication/323786906_Detection_of_tuberculosis_by_automatic_cough_sound_analysis\n",
    "\n",
    "* Use CNN and a dense network for tabular data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499d2f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import gc\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b830cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/media/SSD/CODA-TB/data')\n",
    "solicited = data_dir/'solicited'\n",
    "TRAIN_FOLDER = solicited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f97b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = Path('/media/SSD/CODA-TB/models')\n",
    "\n",
    "os.makedirs(MODEL_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4303e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T02:43:31.952389Z",
     "start_time": "2023-02-13T02:43:31.949953Z"
    }
   },
   "source": [
    "## Load the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5db6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_df = pd.read_csv(data_dir/'metadata'/\"challenge1_metadata.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce80e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_df = pd.read_csv(data_dir/'metadata'/\"challenge2_metadata.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf131728",
   "metadata": {},
   "source": [
    "### Find data with both file and clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25998573",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_cols = ['sex', 'age', 'height', 'weight', 'reported_cough_dur',\n",
    "       'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown',\n",
    "       'hemoptysis', 'heart_rate', 'temperature', 'weight_loss', 'smoke_lweek',\n",
    "       'fever', 'night_sweats']\n",
    "\n",
    "cat_cols = ['sex','tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown',\n",
    "            'hemoptysis', 'weight_loss', 'smoke_lweek','fever', 'night_sweats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75345380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.merge(challenge1_df, challenge2_df[['participant']+tab_cols], on = 'participant', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a234b",
   "metadata": {},
   "source": [
    "### Encode the categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s):\n",
    "    lu = {'Male': -1, 'Female':1, 'Yes':1, 'No':-1, 'Not sure':0}\n",
    "    s = s.apply(lambda x:lu[x])\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71264fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_df = data_df.apply(lambda x: convert(x) if x.name in cat_cols else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f159b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafaef59",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9422a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librosa version\n",
    "class Dataset_from_df(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_df, path, crop_length=22050):\n",
    "        self.file_df = file_df\n",
    "        self.path = path\n",
    "        self.crop_length = crop_length\n",
    "        \n",
    "        self.tab_cols = ['sex', 'age', 'height', 'weight', 'reported_cough_dur',\n",
    "                         'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown',\n",
    "                         'hemoptysis', 'heart_rate', 'temperature', 'weight_loss', 'smoke_lweek',\n",
    "                         'fever', 'night_sweats']\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def _randint(a, b):\n",
    "            return torch.randint(a, b, (1, )).item()\n",
    "        \n",
    "        #meta = torchaudio.info(self.path + \"/\" + df.iloc[index]['subpath'])\n",
    "        target = self.file_df.iloc[index]['tb_status']*1.0\n",
    "        length = self.file_df.iloc[index]['samples']\n",
    "        \n",
    "        # get tabular data\n",
    "        tabular = self.file_df.iloc[index][self.tab_cols].to_numpy(dtype='float32')\n",
    "        \n",
    "        p = self.path/self.file_df.iloc[index]['filename']\n",
    "        #p = self.file_df.iloc[index]['path']\n",
    "        orig_id = self.file_df.iloc[index]['orig_id']\n",
    "\n",
    "        if length == self.crop_length:\n",
    "            x, sr = librosa.load(str(p),sr=None)\n",
    "            \n",
    "        elif length < self.crop_length:\n",
    "            s, sr = librosa.load(str(p),sr=None)\n",
    "\n",
    "            zeros_front = (_randint(0, self.crop_length - length))\n",
    "            pad_front = np.zeros(( zeros_front))\n",
    "            pad_back = np.zeros(( self.crop_length - length - zeros_front))\n",
    "            waveform = np.concatenate((pad_front, s, pad_back), 0)\n",
    "            x = waveform    \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            crop_start = _randint(0, int(length - self.crop_length))\n",
    "            x, sr = librosa.load(str(p),sr=None)\n",
    "            x = x[crop_start:self.crop_length+crop_start]\n",
    "            \n",
    "        # transform sound sample to spectrum\n",
    "        x = np.abs(librosa.stft(x,n_fft = 1024,hop_length=64,window ='hanning'))\n",
    "        x = librosa.amplitude_to_db(x).astype(np.float32)\n",
    "        x = (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "        x = np.flip(x[1:349,:],0).copy() # approx 50Hz to 15000Hz\n",
    "  \n",
    "        \n",
    "        x=torch.from_numpy(x).unsqueeze(0)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "        return {'x':x, 'sr':sr, 'tabular':tabular,'target':torch.FloatTensor([target, 1.0-target]),'orig_id':orig_id}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23823f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(df, fold):\n",
    "    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    \n",
    "    #balance the data set\n",
    "    df_counts = pd.DataFrame(train_df.groupby(['tb_status']).size().reset_index()).rename(columns={0:\"counts\"})\n",
    "    df_counts['weights'] = df_counts['counts'].max() / df_counts['counts']\n",
    "    df_balanced = pd.merge(train_df, df_counts[['tb_status','weights']], on='tb_status')\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(df_balanced['weights'].values, len(df_balanced))\n",
    "\n",
    "    train_dataset = Dataset_from_df(train_df,TRAIN_FOLDER)\n",
    "    valid_dataset = Dataset_from_df(valid_df,TRAIN_FOLDER)\n",
    "    \n",
    "    return train_dataset, valid_dataset, sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421335b8",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelExpand(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelExpand, self).__init__()\n",
    "        self.channels = channels\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x = x.expand(-1,3,*x.shape[2:])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aba52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet34', pretrained=True,drop_rate = .50, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModel, self).__init__()\n",
    "        self.cnn = timm.create_model('resnet34', pretrained=True,drop_rate = .50, num_classes=2)\n",
    "        self.cnn.fc = torch.nn.Linear(\n",
    "            self.cnn.fc.in_features, 20)  #modify last layer to 20 features (from 512)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(20 + 16, 60) # dense layer \n",
    "        self.fc2 = torch.nn.Linear(60, 2) # output layer\n",
    "        \n",
    "        self.to3_channel = ChannelExpand(3)\n",
    "        \n",
    "    def forward(self, image, data):\n",
    "        image = self.to3_channel(image)\n",
    "        x1 = self.cnn(image)\n",
    "        x2 = data\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "om_descr = 'timm-resnet34-c2'\n",
    "\n",
    "def build_model():\n",
    "    # build a new model\n",
    "    model = MultiModel()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffd97b",
   "metadata": {},
   "source": [
    "## Training and Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainfp32(epoch, train_dataloader, optimizer, loss_fn, lr_scheduler,\n",
    "              metric_fn, model):\n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    training_loss = 0\n",
    "    metric = 0\n",
    "\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "    pbar = tqdm(total=len(train_dataloader))\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        x = batch['x']\n",
    "        d = batch['tabular']\n",
    "        sr = batch['sr']\n",
    "\n",
    "        y = batch['target']\n",
    "\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        d = d.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "\n",
    "        output = model(x,d)\n",
    "        \n",
    "        loss = loss_fn(m(output), y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        training_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "        metric += metric_fn(y, output) * x.size(0)\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    training_loss /= len(train_dataloader.dataset)\n",
    "    metric /= len(train_dataloader.dataset)\n",
    "    return training_loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferfp32(valid_dataloader, model):\n",
    "    act = np.zeros(len(validation_dataloader.dataset))\n",
    "    pred = np.zeros(len(validation_dataloader.dataset))\n",
    "    orig_id = np.zeros(len(validation_dataloader.dataset))\n",
    "\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    model.eval()\n",
    "    pbar = tqdm(total=len(valid_dataloader))\n",
    "\n",
    "    st = 0\n",
    "    for batch in valid_dataloader:\n",
    "        x = batch['x']\n",
    "        d = batch['tabular']\n",
    "        sr = batch['sr']\n",
    "        y = batch['target']\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        d = d.cuda(non_blocking=True)\n",
    "        \n",
    "        output = model(x,d)\n",
    "        \n",
    "        \n",
    "        en = st + y.shape[0]\n",
    "        act[st:en] = y[:,0].numpy()\n",
    "        pred[st:en] = m(output)[:,0].cpu().detach().numpy()\n",
    "        orig_id[st:en] = batch['orig_id']\n",
    "        st = en\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    \n",
    "    return act, pred, orig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(loss, metric):\n",
    "    fig, ax = plt.subplots(2,1,figsize=(15,15))\n",
    "    loss_line, = ax[0].plot(loss, label='loss')\n",
    "    metric_line, = ax[1].plot(metric, label='metric')\n",
    "    ax[0].legend(handles=[loss_line])\n",
    "    ax[1].legend(handles=[metric_line])\n",
    "    ax[0].set_title( \"loss\")\n",
    "    ax[1].set_title( \"metric\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee88ea",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def p_auroc(act,pred,fpr_thresh=1.0,tpr_thresh=0.0):\n",
    "    fpr, tpr, _ = roc_curve(act, pred)\n",
    "    #roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    print('auroc=',roc_auc_score(act,pred))\n",
    "    i = fpr <= fpr_thresh\n",
    "    j = tpr >= tpr_thresh\n",
    "    \n",
    "    if len(fpr[i&j]) > 1 :\n",
    "        pauc_approx = auc(fpr[i&j], tpr[i&j]-tpr_thresh)\n",
    "\n",
    "        max_ij = np.argmax(fpr[i&j])\n",
    "        pauc_extra = (fpr_thresh-fpr[i&j][max_ij]) * (tpr[i&j][max_ij]-tpr_thresh)\n",
    "        pauc_better = pauc_approx + pauc_extra\n",
    "    else:\n",
    "        pauc_better = 0\n",
    "\n",
    "    return pauc_better \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, average_precision_score\n",
    "\n",
    "\n",
    "def f1_torch(output, target):\n",
    "    y_pred = output.argmax(axis=1)\n",
    "    y_true = target.argmax(axis=1)\n",
    "    np_true = y_true.to(\"cpu\").to(torch.int).numpy()\n",
    "    np_pred = y_pred.to(\"cpu\").to(torch.int).numpy()\n",
    "    return f1_score(np_true, np_pred, average=\"micro\", zero_division=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03acc70",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca838b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = .0005\n",
    "num_epochs=30\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_folds = 5\n",
    "load_weights = False\n",
    "training_phase = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe59c5e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8500bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(0,num_folds):\n",
    "    \n",
    "    model = build_model()\n",
    "    model.cuda()\n",
    "    \n",
    "    if load_weights:\n",
    "        model.load_state_dict(torch.load(MODEL_FOLDER / f\"{fold}_dict_{om_descr}_{training_phase}.pth\"))\n",
    "    \n",
    "    train_dataset, valid_dataset, sampler = prepare_datasets(challenge2_df,fold)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                   batch_size=32,\n",
    "                                                   num_workers=10,\n",
    "                                                   pin_memory=True,\n",
    "                                                   shuffle=False,\n",
    "                                                   sampler=sampler)\n",
    "\n",
    "    validation_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                       batch_size=32,\n",
    "                                                       num_workers=10,\n",
    "                                                       pin_memory=True,\n",
    "                                                       shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=init_lr,\n",
    "                                                       steps_per_epoch=len(train_dataloader),epochs=num_epochs)\n",
    "    \n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "\n",
    "    best_metric = -1\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"fold {fold + 1}/{num_folds}\")    \n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        lr_val = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Fold:{fold} epoch {epoch}/{num_epochs}  LR={lr_val}\")\n",
    "\n",
    "        training_loss, training_metric = trainfp32(epoch,\n",
    "                                                   train_dataloader,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   lr_scheduler,\n",
    "                                                   metric_fn=f1_torch,\n",
    "                                                   model=model)\n",
    "\n",
    "        print('Training-- Loss: {:.4f}, Metric: {:.3f}'.format(\n",
    "            training_loss, training_metric),\n",
    "              end=',')\n",
    "        act, pred, _= inferfp32(validation_dataloader, model)\n",
    "        \n",
    "        p_auroc_metric = p_auroc(act,pred,fpr_thresh=0.4,tpr_thresh=0.8)\n",
    "        auroc_metric = roc_auc_score(act,pred)\n",
    "        f1_metric = f1_score(act,pred>0.5, average=\"micro\", zero_division=0)\n",
    "        \n",
    "        validation_metric = auroc_metric\n",
    "\n",
    "\n",
    "        print(f'Validation--  Metrics: auroc={auroc_metric:.3f}, p_auroc={p_auroc_metric:.5f}, f1={f1_metric:.3f}',end='\\n')\n",
    "\n",
    "        epoch_loss_values.append(training_loss)\n",
    "        metric_values.append(validation_metric)\n",
    "        \n",
    "        if validation_metric > best_metric:\n",
    "            best_metric = validation_metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), MODEL_FOLDER / f\"{fold}_best_dict_{om_descr}_{training_phase}.pth\")\n",
    "            print(\"saved new best metric model\")        \n",
    " \n",
    "    plot_metrics(epoch_loss_values, metric_values)\n",
    "    torch.save(model.state_dict(), MODEL_FOLDER / f\"{fold}_dict_{om_descr}_{training_phase}.pth\")\n",
    "\n",
    "    model.cpu()\n",
    "    del model, optimizer, train_dataloader, validation_dataloader, lr_scheduler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f207b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aba4cb",
   "metadata": {},
   "source": [
    "### Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights = True\n",
    "training_phase = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.zeros(len(challenge1_df))\n",
    "pred = np.zeros(len(challenge1_df))\n",
    "orig_id = np.zeros(len(challenge1_df))\n",
    "num_folds = 5\n",
    "st = 0\n",
    "\n",
    "for fold in range(0,num_folds):\n",
    "    \n",
    "    model = build_model()\n",
    "    model.cuda()\n",
    "    \n",
    "\n",
    "    model.load_state_dict(torch.load(MODEL_FOLDER / f\"{fold}_best_dict_{om_descr}_{training_phase}.pth\"))\n",
    "    \n",
    "    train_dataset, valid_dataset, _ = prepare_datasets(challenge2_df,fold)\n",
    "    \n",
    "\n",
    "    validation_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                       batch_size=16,\n",
    "                                                       num_workers=12,\n",
    "                                                       pin_memory=True,\n",
    "                                                       shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    #get real data as dummy for ONNX\n",
    "    iterator_loader = iter(validation_dataloader)\n",
    "    batch = next(iterator_loader)\n",
    "    dummy_input1 = batch['x']\n",
    "    dummy_input2 = batch['tabular']\n",
    "    dummy_input = (dummy_input1,dummy_input2 )\n",
    "    \n",
    "    filename = f\"{fold}_best_dict_{om_descr}_{training_phase}.onnx\"\n",
    "    file_path = MODEL_FOLDER / filename\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "    # save the network\n",
    "    torch.onnx.export(model,         # model being run \n",
    "         dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "         file_path,       # where to save the model  \n",
    "         export_params=True,  # store the trained parameter weights inside the model file \n",
    "         opset_version=10,    # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['modelInput1','modelInput2'],   # the model's input names \n",
    "         output_names = ['modelOutput'], # the model's output names \n",
    "         dynamic_axes={'modelInput1' : {0 : 'batch_size'},'modelInput2' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'modelOutput' : {0 : 'batch_size'}}) \n",
    "    \n",
    "   \n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    #Convert_ONNX(model,dummy_input,file_path)\n",
    "    \n",
    "    act_fold,pred_fold,orig_id_fold = inferfp32(validation_dataloader, model)\n",
    "\n",
    "    en = st + act_fold.shape[0]\n",
    "    act[st:en] = act_fold\n",
    "    pred[st:en] = pred_fold\n",
    "    orig_id[st:en] = orig_id_fold\n",
    "    st = en\n",
    "\n",
    "\n",
    "    model.cpu()\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = pd.DataFrame({'orig_id':orig_id, 'act':act,'pred':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df[inference_df['act'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = pd.merge(inference_df,challenge2_df, left_on='orig_id', right_index=True)\n",
    "inference_df.drop(['orig_id_x','orig_id_y'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ac267",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = inference_df.groupby('participant')[['pred','act']].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('inference_ch2_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mean values as a quick way to get predicted and actual by participant\n",
    "pred = res_df['pred'].values\n",
    "act = res_df['act'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e84e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auroc_confidence import auc_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, auroc_bs_score, auroc_bs_score_ci, _, _ = auc_metric.AUC_CI_Bootstrap(act, pred, CI_index=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681344e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauroc_metric = p_auroc(act,pred,fpr_thresh=0.4,tpr_thresh=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520afdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(act,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb427c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(act,(pred>0.5), average=\"micro\", zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cafc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = auroc_bs_score\n",
    "b = auroc_bs_score_ci[0]\n",
    "c = auroc_bs_score_ci[1]\n",
    "\n",
    "print(f\"Validation AUROC:{a:.3f} , ci=[{b:.3f}, {c:.3f}]\")\n",
    "print(f\"Validation P-AUROC:{pauroc_metric:.3f}\")\n",
    "print(f\"F1 Validation Score:{f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b3f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
